{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "# Random\n",
    "import random\n",
    "random.seed = 42\n",
    "# Thread management (sleep purposes)\n",
    "import time\n",
    "# Operating System management\n",
    "import os\n",
    "import selenium\n",
    "# Selenium specific libraries\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import numpy as np\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(requests.get('https://josemamerto.blogspot.com/search?updated-max=2024-05-06T00:51:00-07:00&max-results=11').content, 'html.parser')\n",
    "\n",
    "titles = []\n",
    "links = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Espacios Vectoriales\n",
      "\n",
      "https://josemamerto.blogspot.com/2024/05/espacios-vectoriales.html\n",
      "----------------------\n",
      "\n",
      "Pero esto qué es\n",
      "\n",
      "https://josemamerto.blogspot.com/2024/05/pero-esto-que-es.html\n",
      "----------------------\n",
      "\n",
      "El pelucas\n",
      "\n",
      "https://josemamerto.blogspot.com/2024/03/blog-post.html\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "titulos = soup.find_all('h3', class_='post-title entry-title')\n",
    "for i in titulos:\n",
    "    print(i.text)\n",
    "    print(i.a['href'])\n",
    "    print('----------------------')\n",
    "    titles.append(i.text)\n",
    "    links.append(i.a['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP (y IV) Creando un Buscador de Pisos con Similitud de Coseno y modelo LLaMA\n",
      "\n",
      "https://josemamerto.blogspot.com/2024/05/nlp-y-iv-creando-un-buscador-de-pisos.html\n",
      "----------------------\n",
      "\n",
      "NLP (III)  Explorando Temas Ocultos: Topic Modeling y Word Embeddings\n",
      "\n",
      "https://josemamerto.blogspot.com/2024/05/nlp-iii-explorando-temas-ocultos-topic.html\n",
      "----------------------\n",
      "\n",
      "NLP (II) Domando los Datos: Preprocesamiento y Almacenamiento en DataFrame\n",
      "\n",
      "https://josemamerto.blogspot.com/2024/05/nlp-ii-domando-los-datos.html\n",
      "----------------------\n",
      "\n",
      "NLP (I) Web Scraping\n",
      "\n",
      "https://josemamerto.blogspot.com/2024/05/nlp-i-web-scraping.html\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(requests.get('https://josemamerto.blogspot.com/').content, 'html.parser')\n",
    "\n",
    "text = soup.find('h3', class_='post-title').text\n",
    "enlace = soup.find('h3', class_='post-title').a['href']\n",
    "print(text)\n",
    "print(f'\\n{enlace}')\n",
    "print('----------------------')\n",
    "titles.append(text)\n",
    "links.append(enlace)    \n",
    "\n",
    "texts = soup.find_all('h3', class_='post-title entry-title')\n",
    "\n",
    "for i in texts:\n",
    "    print(i.text)\n",
    "    print(i.a['href'])\n",
    "    print('----------------------')\n",
    "    titles.append(i.text)\n",
    "    links.append(i.a['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limpiar titles, eliminar saltos de linea\n",
    "titles = [re.sub('\\n', '', i) for i in titles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "textos = {titulo: [] for titulo in titles}\n",
    "for titulo, link in zip(titles, links):\n",
    "    soup = BeautifulSoup(requests.get(f'{link}').content, 'html.parser')\n",
    "    articulo_div = soup.find('div', class_ = 'post-body entry-content float-container')\n",
    "    articulos = articulo_div.find_all('p')  # find all 'p' tags in the div\n",
    "    for articulo in articulos:\n",
    "        textos[titulo].append(articulo.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "for titulo in textos:\n",
    "    textos[titulo] = ' '.join(textos[titulo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Espacios Vectoriales</td>\n",
       "      <td>Explorando los Espacios Vectoriales: Fundamen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pero esto qué es</td>\n",
       "      <td>Acá en este bloggaso vamos a presentar unos d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>El pelucas</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NLP (y IV) Creando un Buscador de Pisos con Si...</td>\n",
       "      <td>En esta última parte de nuestro análisis, cons...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NLP (III)  Explorando Temas Ocultos: Topic Mod...</td>\n",
       "      <td>En el análisis de texto, el topic modeling es ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NLP (II) Domando los Datos: Preprocesamiento y...</td>\n",
       "      <td>En el fascinante mundo del análisis de datos,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NLP (I) Web Scraping</td>\n",
       "      <td>¿Alguna vez te has preguntado cómo se recopil...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0                               Espacios Vectoriales   \n",
       "1                                   Pero esto qué es   \n",
       "2                                         El pelucas   \n",
       "3  NLP (y IV) Creando un Buscador de Pisos con Si...   \n",
       "4  NLP (III)  Explorando Temas Ocultos: Topic Mod...   \n",
       "5  NLP (II) Domando los Datos: Preprocesamiento y...   \n",
       "6                               NLP (I) Web Scraping   \n",
       "\n",
       "                                                Text  \n",
       "0   Explorando los Espacios Vectoriales: Fundamen...  \n",
       "1   Acá en este bloggaso vamos a presentar unos d...  \n",
       "2                                                     \n",
       "3  En esta última parte de nuestro análisis, cons...  \n",
       "4  En el análisis de texto, el topic modeling es ...  \n",
       "5   En el fascinante mundo del análisis de datos,...  \n",
       "6   ¿Alguna vez te has preguntado cómo se recopil...  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(list(textos.items()), columns=['Title', 'Text'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import unicodedata\n",
    "\n",
    "nlp=spacy.load('es_core_news_sm')\n",
    "\n",
    "def normalize_document(doc):\n",
    "\n",
    "    doc = unicodedata.normalize('NFKD', doc).encode('ASCII', 'ignore').decode('utf-8')\n",
    "\n",
    "    # tokenizamos el texto\n",
    "    tokens = nlp(doc)\n",
    "    # quitamos puntuación/espacios/stop words y cogemos el lema\n",
    "    lemmas = [t.lemma_ for t in tokens if not t.is_punct and not t.is_space and not t.is_stop]\n",
    "    doc = ' '.join(lemmas)\n",
    "    return doc\n",
    "\n",
    "def normalize_corpus(corpus):\n",
    "    \"\"\"Normaliza un corpus de documentos aplicando al función de normalización\n",
    "    normalize_document() a cada documento de la lista pasada como argumento\"\"\"   \n",
    "    return [normalize_document(text) for text in corpus]\n",
    "\n",
    "corpus = df['Text'].tolist()\n",
    "corpus = normalize_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# usamos características tf-idf para LSA.\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=2)\n",
    "tfidf = tfidf_vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    \"\"\"Función auxiliar para mostrar los términos más importantes\n",
    "    de cada topic\"\"\"\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = f\"Topic #{topic_idx}: \"\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics en modelo LSA:\n",
      "Topic #0: dato piso paso web scraping\n",
      "Topic #1: word texto paso embeddings analisis\n",
      "Topic #2: vector propiedad aplicación conjunto word\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation\n",
    "\n",
    "# Ajustamos el modelo LSA\n",
    "lsa = TruncatedSVD(n_components=3).fit(tfidf)\n",
    "\n",
    "print(\"\\nTopics en modelo LSA:\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "print_top_words(lsa, tfidf_feature_names, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>Topic 0</th>\n",
       "      <th>Topic 1</th>\n",
       "      <th>Topic 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Espacios Vectoriales</td>\n",
       "      <td>Explorando los Espacios Vectoriales: Fundamen...</td>\n",
       "      <td>0.3631</td>\n",
       "      <td>-0.5178</td>\n",
       "      <td>0.6996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pero esto qué es</td>\n",
       "      <td>Acá en este bloggaso vamos a presentar unos d...</td>\n",
       "      <td>0.6585</td>\n",
       "      <td>-0.4431</td>\n",
       "      <td>-0.1216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>El pelucas</td>\n",
       "      <td></td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NLP (y IV) Creando un Buscador de Pisos con Si...</td>\n",
       "      <td>En esta última parte de nuestro análisis, cons...</td>\n",
       "      <td>0.6331</td>\n",
       "      <td>0.0632</td>\n",
       "      <td>0.0645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NLP (III)  Explorando Temas Ocultos: Topic Mod...</td>\n",
       "      <td>En el análisis de texto, el topic modeling es ...</td>\n",
       "      <td>0.5704</td>\n",
       "      <td>0.6455</td>\n",
       "      <td>0.3228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NLP (II) Domando los Datos: Preprocesamiento y...</td>\n",
       "      <td>En el fascinante mundo del análisis de datos,...</td>\n",
       "      <td>0.7538</td>\n",
       "      <td>0.2713</td>\n",
       "      <td>-0.1401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NLP (I) Web Scraping</td>\n",
       "      <td>¿Alguna vez te has preguntado cómo se recopil...</td>\n",
       "      <td>0.5397</td>\n",
       "      <td>-0.2464</td>\n",
       "      <td>-0.5435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0                               Espacios Vectoriales   \n",
       "1                                   Pero esto qué es   \n",
       "2                                         El pelucas   \n",
       "3  NLP (y IV) Creando un Buscador de Pisos con Si...   \n",
       "4  NLP (III)  Explorando Temas Ocultos: Topic Mod...   \n",
       "5  NLP (II) Domando los Datos: Preprocesamiento y...   \n",
       "6                               NLP (I) Web Scraping   \n",
       "\n",
       "                                                Text  Topic 0  Topic 1  \\\n",
       "0   Explorando los Espacios Vectoriales: Fundamen...   0.3631  -0.5178   \n",
       "1   Acá en este bloggaso vamos a presentar unos d...   0.6585  -0.4431   \n",
       "2                                                      0.0000   0.0000   \n",
       "3  En esta última parte de nuestro análisis, cons...   0.6331   0.0632   \n",
       "4  En el análisis de texto, el topic modeling es ...   0.5704   0.6455   \n",
       "5   En el fascinante mundo del análisis de datos,...   0.7538   0.2713   \n",
       "6   ¿Alguna vez te has preguntado cómo se recopil...   0.5397  -0.2464   \n",
       "\n",
       "   Topic 2  \n",
       "0   0.6996  \n",
       "1  -0.1216  \n",
       "2   0.0000  \n",
       "3   0.0645  \n",
       "4   0.3228  \n",
       "5  -0.1401  \n",
       "6  -0.5435  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assuming lsa and tfidf are defined and have the correct dimensions\n",
    "topic_values = np.round(lsa.transform(tfidf), 4)\n",
    "\n",
    "# convert the array to a DataFrame\n",
    "topic_df = pd.DataFrame(topic_values, columns=['Topic 0', 'Topic 1', 'Topic 2'])\n",
    "\n",
    "# concatenate the DataFrames\n",
    "df = pd.concat([df, topic_df], axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://josemamerto.blogspot.com/2024/05/espacios-vectoriales.html\n",
      "alex es mi lider\n",
      "ché me cague a trompadas de risa con el content, publicá más xfiiiiiiiii\n",
      "----------------------\n",
      "https://josemamerto.blogspot.com/2024/05/espacios-vectoriales.html\n",
      "Elseroculto\n",
      "空间向量是线性代数研究的基石，这一学科在物理学、工程学、计算机科学和经济学等领域有广泛的应用。在这篇博客中，我们将深入探讨空间向量的世界，探索其基础、特性和应用。什么是空间向量？空间向量是一种数学结构，由一组称为向量的元素以及两种操作组成，即向量加法和标量乘法，满足某些基本性质。正式地，空间向量𝑉V 在域𝐹F 上是一个非空元素集合，称为向量，以及两种在𝑉V 上定义的操作：向量加法和标量乘法，这些操作满足八条公理，规定了它们必须满足的基本性质。特性和示例空间向量可以具有多种特性，其中一些包括对加法和标量乘法封闭、存在加法单位元素、存在加法逆元素和操作的结合律。常见的空间向量示例包括欧几里得空间𝑅𝑛，其中包含所有n-维向量，其坐标为实数，以及𝑃𝑛多项式空间，其多项式的次数不超过n。在现实生活中的应用空间向量在现实世界中有广泛的应用。在物理学中，它们用于建模力、力矩和电磁场。在工程学中，它们应用于结构分析、电路和控制系统。在计算机科学中，它们用于图像处理、计算机图形学和机器学习。甚至在经济学中，它们用于建模经济变量之间的关系和优化财务决策。高级特性和定理除了基本性质外，空间向量还表现出一系列更高级的特性和重要定理。例如，共同基础定理规定所有空间向量都有一个基础，即一组线性独立的向量，它们生成整个空间。秩定理告诉我们矩阵的秩等于可以从其列中获得的线性独立向量的最大数量。结论空间向量是数学和应用科学中强大而多才多艺的工具。它们的研究不仅提供了对空间结构和线性变换的深入理解，而且在各种领域都有实际应用。无论是对亚原子粒子运动建模还是设计人工智能算法，空间向量仍然是我们对周围世界理解的不可或缺的一部分。希望这次探索给您带来了对空间向量重要性和美妙之处的清晰认识！如果您有任何问题或评论，请随时分享！\n",
      "----------------------\n",
      "https://josemamerto.blogspot.com/2024/05/pero-esto-que-es.html\n",
      "Elseroculto\n",
      "Si quieren contratar los servicios de este sicario llamen al número que aparece a continuación 922 999 999 999 999\n",
      "----------------------\n",
      "https://josemamerto.blogspot.com/2024/03/blog-post.html\n",
      "Elseroculto\n",
      "Cara de asesino en serie pero recomendable muy buenos servicios.\n",
      "----------------------\n",
      "https://josemamerto.blogspot.com/2024/03/blog-post.html\n",
      "elkebabesgrande\n",
      "Quien pudiera\n",
      "----------------------\n",
      "https://josemamerto.blogspot.com/2024/03/blog-post.html\n",
      "Carlos Peris\n",
      "Que miedo!\n",
      "----------------------\n",
      "https://josemamerto.blogspot.com/2024/03/blog-post.html\n",
      "Slanc\n",
      "¿Estas siendo internacionalmente buscado por la INTERPOL?\n",
      "----------------------\n",
      "https://josemamerto.blogspot.com/2024/03/blog-post.html\n",
      "Mr_pollon2000\n",
      "Buen articulo sobre el web scraping, un saludo de tu amigo Scalving\n",
      "----------------------\n",
      "https://josemamerto.blogspot.com/2024/05/nlp-y-iv-creando-un-buscador-de-pisos.html\n",
      "Elseroculto\n",
      "¡Este proyecto suena emocionante! Crear un buscador de pisos que utilice la similitud de coseno para comparar la entrada del usuario con las descripciones disponibles es una excelente manera de ofrecer resultados relevantes y personalizados. Además, la integración con la API para automatizar el proceso es un gran paso hacia la eficiencia.El uso de la similitud de coseno es inteligente, ya que permite una comparación precisa entre la consulta del usuario y las descripciones de los pisos. Y la integración del modelo LLaMA para el procesamiento de datos agrega una capa adicional de sofisticación al filtrar la información y crear word embeddings para evaluar la similitud.En resumen, has creado una herramienta poderosa que no solo facilitará la búsqueda de viviendas para los usuarios, sino que también les proporcionará resultados relevantes y precisos. ¡Excelente trabajo! 😊🏠\n",
      "----------------------\n",
      "https://josemamerto.blogspot.com/2024/05/nlp-iii-explorando-temas-ocultos-topic.html\n",
      "Elseroculto\n",
      "¡Este artículo sobre NLP es muy informativo y ofrece una visión fascinante sobre el uso del topic modeling y los word embeddings para analizar datos de texto!El topic modeling es una técnica increíblemente útil para descubrir temas subyacentes en conjuntos de documentos, lo que puede proporcionar insights valiosos sobre los datos. La forma en que el artículo lo presenta y explica cómo funciona es muy clara y comprensible.Además, la incorporación de los word embeddings mediante Word2Vec es una adición excelente. Estos embeddings permiten representar palabras de manera semántica, lo que enriquece aún más el análisis de texto al capturar las relaciones semánticas entre las palabras.El hecho de que el artículo incluya ejemplos de código para implementar estas técnicas en Python usando bibliotecas como Gensim es muy útil para aquellos que deseen aplicar estos métodos en sus propios proyectos de NLP.En resumen, este artículo proporciona una introducción sólida al topic modeling y los word embeddings, y muestra cómo estas técnicas pueden ser aplicadas de manera efectiva para comprender y analizar datos de texto de manera más profunda. ¡Excelente trabajo! 🌟\n",
      "----------------------\n",
      "https://josemamerto.blogspot.com/2024/05/nlp-ii-domando-los-datos.html\n",
      "Samuel\n",
      "¡Hola! Me encantó tu artículo sobre el preprocesamiento y almacenamiento de datos en un DataFrame utilizando Python y pandas. El proceso paso a paso que proporcionaste es muy claro y útil para cualquiera que esté comenzando en el mundo del análisis de datos.Me gustó especialmente cómo abordaste la limpieza de datos y el preprocesamiento de texto, mostrando cómo eliminar valores nulos, duplicados y caracteres especiales, así como convertir el texto a minúsculas y eliminar palabras vacías. Estos son pasos esenciales para garantizar la calidad de los datos antes de realizar cualquier análisis.Además, la sección sobre la importación de un archivo pickle y su transformación en un DataFrame de pandas fue muy informativa. La forma en que explicaste cada paso y proporcionaste ejemplos de código fue muy útil para comprender el proceso.En general, creo que tu artículo proporciona una excelente introducción al preprocesamiento de datos y el manejo de DataFrames en Python, y estoy seguro de que será de gran ayuda para muchos lectores interesados en aprender análisis de datos. ¡Espero con ansias tu próximo artículo!\n",
      "----------------------\n",
      "https://josemamerto.blogspot.com/2024/05/nlp-ii-domando-los-datos.html\n",
      "lentejas\n",
      "como dijo Mariano Rajoy: it's very difficult todo esto\n",
      "----------------------\n",
      "https://josemamerto.blogspot.com/2024/05/nlp-ii-domando-los-datos.html\n",
      "Chegio\n",
      "Segun la teoría del Big Crunch, con el tiempo, tras el Big Bang, todo el universo volverá a unirse en solo punto como estaba antes del Big Bang. Esto podría abogar a un universo \"pulsante\" donde surgiesen diversos Big Bangs continuamente tras los Big Crunchs. Si además sugerimos un universo determinista, esto implicaría que podriamos determinar cualquier suceso del universo sabiendo lo que ya ha pasado. Por lo que nuestras acciones serían fijas y podrian predecirse. La implicación de este detreminismo haría en todos y cada uno de los universos tomaramos las mismas decisiones y repitiesemos la misma vida una y otra vez. Y en todos y cada uno de estos infinitos universos se toma la decisión de crear este magnifico Blog, muchas gracias.\n",
      "----------------------\n",
      "https://josemamerto.blogspot.com/2024/05/nlp-i-web-scraping.html\n",
      "Samuel\n",
      "¡Qué artículo tan fascinante sobre el web scraping! Me encantó cómo proporcionaste una visión completa de esta técnica increíble que nos permite extraer datos de la web de manera automatizada. El mundo del web scraping es realmente emocionante y lleno de posibilidades para explorar y aprovechar la vasta cantidad de información disponible en Internet.Tu explicación sobre qué es el web scraping y cómo funciona fue muy clara y comprensible. Me gustó especialmente cómo diste ejemplos prácticos, como buscar información sobre el clima en diferentes ciudades, para ilustrar el proceso. También aprecio que mencionaras las herramientas y tecnologías más populares utilizadas en el web scraping, como BeautifulSoup y Scrapy, lo que hace que sea más accesible para aquellos que estén interesados en probarlo.Además, tu artículo abordó de manera adecuada las consideraciones éticas y legales asociadas con el web scraping. Es crucial respetar los términos de servicio de los sitios web y obtener permiso antes de extraer datos, y aprecio que hayas destacado este punto.Por último, tu explicación sobre cómo implementaste el web scraping utilizando las bibliotecas Selenium y BeautifulSoup para acceder a las páginas web y extraer información fue muy informativa. Proporcionaste una visión interna de cómo se realiza el proceso en la práctica, lo que ayuda a comprender mejor el alcance y las posibilidades del web scraping.En resumen, tu artículo proporciona una excelente introducción al mundo del web scraping y ofrece una visión completa de su funcionamiento, aplicaciones y consideraciones éticas. ¡Espero ver más contenido como este en el futuro!\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "keys = links\n",
    "comentarios = {key: {} for key in keys}\n",
    "\n",
    "for i in keys:\n",
    "    soup = BeautifulSoup(requests.get(f'{i}').content, 'html.parser')\n",
    "    comentario = soup.find_all('li', class_ = 'comment')\n",
    "    for j in comentario:\n",
    "        try :\n",
    "            comments = j.find('p', class_ = 'comment-content')\n",
    "            user = j.find('cite', class_ = 'user')\n",
    "            print(i)\n",
    "            print(user.text)\n",
    "            print(comments.text)\n",
    "            print('----------------------')\n",
    "            comentarios[i][user.text] = comments.text\n",
    "        except AttributeError:\n",
    "            comments = 'No hay comentarios'\n",
    "            print(i)\n",
    "            print(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'https://josemamerto.blogspot.com/2024/05/espacios-vectoriales.html': {'alex es mi lider': 'ché me cague a trompadas de risa con el content, publicá más xfiiiiiiiii',\n",
       "  'Elseroculto': '空间向量是线性代数研究的基石，这一学科在物理学、工程学、计算机科学和经济学等领域有广泛的应用。在这篇博客中，我们将深入探讨空间向量的世界，探索其基础、特性和应用。什么是空间向量？空间向量是一种数学结构，由一组称为向量的元素以及两种操作组成，即向量加法和标量乘法，满足某些基本性质。正式地，空间向量𝑉V 在域𝐹F 上是一个非空元素集合，称为向量，以及两种在𝑉V 上定义的操作：向量加法和标量乘法，这些操作满足八条公理，规定了它们必须满足的基本性质。特性和示例空间向量可以具有多种特性，其中一些包括对加法和标量乘法封闭、存在加法单位元素、存在加法逆元素和操作的结合律。常见的空间向量示例包括欧几里得空间𝑅𝑛，其中包含所有n-维向量，其坐标为实数，以及𝑃𝑛多项式空间，其多项式的次数不超过n。在现实生活中的应用空间向量在现实世界中有广泛的应用。在物理学中，它们用于建模力、力矩和电磁场。在工程学中，它们应用于结构分析、电路和控制系统。在计算机科学中，它们用于图像处理、计算机图形学和机器学习。甚至在经济学中，它们用于建模经济变量之间的关系和优化财务决策。高级特性和定理除了基本性质外，空间向量还表现出一系列更高级的特性和重要定理。例如，共同基础定理规定所有空间向量都有一个基础，即一组线性独立的向量，它们生成整个空间。秩定理告诉我们矩阵的秩等于可以从其列中获得的线性独立向量的最大数量。结论空间向量是数学和应用科学中强大而多才多艺的工具。它们的研究不仅提供了对空间结构和线性变换的深入理解，而且在各种领域都有实际应用。无论是对亚原子粒子运动建模还是设计人工智能算法，空间向量仍然是我们对周围世界理解的不可或缺的一部分。希望这次探索给您带来了对空间向量重要性和美妙之处的清晰认识！如果您有任何问题或评论，请随时分享！'},\n",
       " 'https://josemamerto.blogspot.com/2024/05/pero-esto-que-es.html': {'Elseroculto': 'Si quieren contratar los servicios de este sicario llamen al número que aparece a continuación 922 999 999 999 999'},\n",
       " 'https://josemamerto.blogspot.com/2024/03/blog-post.html': {'Elseroculto': 'Cara de asesino en serie pero recomendable muy buenos servicios.',\n",
       "  'elkebabesgrande': 'Quien pudiera',\n",
       "  'Carlos Peris': 'Que miedo!',\n",
       "  'Slanc': '¿Estas siendo internacionalmente buscado por la INTERPOL?',\n",
       "  'Mr_pollon2000': 'Buen articulo sobre el web scraping, un saludo de tu amigo Scalving'},\n",
       " 'https://josemamerto.blogspot.com/2024/05/nlp-y-iv-creando-un-buscador-de-pisos.html': {'Elseroculto': '¡Este proyecto suena emocionante! Crear un buscador de pisos que utilice la similitud de coseno para comparar la entrada del usuario con las descripciones disponibles es una excelente manera de ofrecer resultados relevantes y personalizados. Además, la integración con la API para automatizar el proceso es un gran paso hacia la eficiencia.El uso de la similitud de coseno es inteligente, ya que permite una comparación precisa entre la consulta del usuario y las descripciones de los pisos. Y la integración del modelo LLaMA para el procesamiento de datos agrega una capa adicional de sofisticación al filtrar la información y crear word embeddings para evaluar la similitud.En resumen, has creado una herramienta poderosa que no solo facilitará la búsqueda de viviendas para los usuarios, sino que también les proporcionará resultados relevantes y precisos. ¡Excelente trabajo! 😊🏠'},\n",
       " 'https://josemamerto.blogspot.com/2024/05/nlp-iii-explorando-temas-ocultos-topic.html': {'Elseroculto': '¡Este artículo sobre NLP es muy informativo y ofrece una visión fascinante sobre el uso del topic modeling y los word embeddings para analizar datos de texto!El topic modeling es una técnica increíblemente útil para descubrir temas subyacentes en conjuntos de documentos, lo que puede proporcionar insights valiosos sobre los datos. La forma en que el artículo lo presenta y explica cómo funciona es muy clara y comprensible.Además, la incorporación de los word embeddings mediante Word2Vec es una adición excelente. Estos embeddings permiten representar palabras de manera semántica, lo que enriquece aún más el análisis de texto al capturar las relaciones semánticas entre las palabras.El hecho de que el artículo incluya ejemplos de código para implementar estas técnicas en Python usando bibliotecas como Gensim es muy útil para aquellos que deseen aplicar estos métodos en sus propios proyectos de NLP.En resumen, este artículo proporciona una introducción sólida al topic modeling y los word embeddings, y muestra cómo estas técnicas pueden ser aplicadas de manera efectiva para comprender y analizar datos de texto de manera más profunda. ¡Excelente trabajo! 🌟'},\n",
       " 'https://josemamerto.blogspot.com/2024/05/nlp-ii-domando-los-datos.html': {'Samuel': '¡Hola! Me encantó tu artículo sobre el preprocesamiento y almacenamiento de datos en un DataFrame utilizando Python y pandas. El proceso paso a paso que proporcionaste es muy claro y útil para cualquiera que esté comenzando en el mundo del análisis de datos.Me gustó especialmente cómo abordaste la limpieza de datos y el preprocesamiento de texto, mostrando cómo eliminar valores nulos, duplicados y caracteres especiales, así como convertir el texto a minúsculas y eliminar palabras vacías. Estos son pasos esenciales para garantizar la calidad de los datos antes de realizar cualquier análisis.Además, la sección sobre la importación de un archivo pickle y su transformación en un DataFrame de pandas fue muy informativa. La forma en que explicaste cada paso y proporcionaste ejemplos de código fue muy útil para comprender el proceso.En general, creo que tu artículo proporciona una excelente introducción al preprocesamiento de datos y el manejo de DataFrames en Python, y estoy seguro de que será de gran ayuda para muchos lectores interesados en aprender análisis de datos. ¡Espero con ansias tu próximo artículo!',\n",
       "  'lentejas': \"como dijo Mariano Rajoy: it's very difficult todo esto\",\n",
       "  'Chegio': 'Segun la teoría del Big Crunch, con el tiempo, tras el Big Bang, todo el universo volverá a unirse en solo punto como estaba antes del Big Bang. Esto podría abogar a un universo \"pulsante\" donde surgiesen diversos Big Bangs continuamente tras los Big Crunchs. Si además sugerimos un universo determinista, esto implicaría que podriamos determinar cualquier suceso del universo sabiendo lo que ya ha pasado. Por lo que nuestras acciones serían fijas y podrian predecirse. La implicación de este detreminismo haría en todos y cada uno de los universos tomaramos las mismas decisiones y repitiesemos la misma vida una y otra vez. Y en todos y cada uno de estos infinitos universos se toma la decisión de crear este magnifico Blog, muchas gracias.'},\n",
       " 'https://josemamerto.blogspot.com/2024/05/nlp-i-web-scraping.html': {'Samuel': '¡Qué artículo tan fascinante sobre el web scraping! Me encantó cómo proporcionaste una visión completa de esta técnica increíble que nos permite extraer datos de la web de manera automatizada. El mundo del web scraping es realmente emocionante y lleno de posibilidades para explorar y aprovechar la vasta cantidad de información disponible en Internet.Tu explicación sobre qué es el web scraping y cómo funciona fue muy clara y comprensible. Me gustó especialmente cómo diste ejemplos prácticos, como buscar información sobre el clima en diferentes ciudades, para ilustrar el proceso. También aprecio que mencionaras las herramientas y tecnologías más populares utilizadas en el web scraping, como BeautifulSoup y Scrapy, lo que hace que sea más accesible para aquellos que estén interesados en probarlo.Además, tu artículo abordó de manera adecuada las consideraciones éticas y legales asociadas con el web scraping. Es crucial respetar los términos de servicio de los sitios web y obtener permiso antes de extraer datos, y aprecio que hayas destacado este punto.Por último, tu explicación sobre cómo implementaste el web scraping utilizando las bibliotecas Selenium y BeautifulSoup para acceder a las páginas web y extraer información fue muy informativa. Proporcionaste una visión interna de cómo se realiza el proceso en la práctica, lo que ayuda a comprender mejor el alcance y las posibilidades del web scraping.En resumen, tu artículo proporciona una excelente introducción al mundo del web scraping y ofrece una visión completa de su funcionamiento, aplicaciones y consideraciones éticas. ¡Espero ver más contenido como este en el futuro!'}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comentarios"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
