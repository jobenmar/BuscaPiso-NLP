{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "# Random\n",
    "import random\n",
    "random.seed = 42\n",
    "# Thread management (sleep purposes)\n",
    "import time\n",
    "# Operating System management\n",
    "import os\n",
    "import selenium\n",
    "# Selenium specific libraries\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import numpy as np\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(requests.get('https://josemamerto.blogspot.com/search?updated-max=2024-05-06T00:51:00-07:00&max-results=11').content, 'html.parser')\n",
    "\n",
    "titles = []\n",
    "links = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Espacios Vectoriales\n",
      "\n",
      "https://josemamerto.blogspot.com/2024/05/espacios-vectoriales.html\n",
      "----------------------\n",
      "\n",
      "Pero esto quÃ© es\n",
      "\n",
      "https://josemamerto.blogspot.com/2024/05/pero-esto-que-es.html\n",
      "----------------------\n",
      "\n",
      "El pelucas\n",
      "\n",
      "https://josemamerto.blogspot.com/2024/03/blog-post.html\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "titulos = soup.find_all('h3', class_='post-title entry-title')\n",
    "for i in titulos:\n",
    "    print(i.text)\n",
    "    print(i.a['href'])\n",
    "    print('----------------------')\n",
    "    titles.append(i.text)\n",
    "    links.append(i.a['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP (y IV) Creando un Buscador de Pisos con Similitud de Coseno y modelo LLaMA\n",
      "\n",
      "https://josemamerto.blogspot.com/2024/05/nlp-y-iv-creando-un-buscador-de-pisos.html\n",
      "----------------------\n",
      "\n",
      "NLP (III)  Explorando Temas Ocultos: Topic Modeling y Word Embeddings\n",
      "\n",
      "https://josemamerto.blogspot.com/2024/05/nlp-iii-explorando-temas-ocultos-topic.html\n",
      "----------------------\n",
      "\n",
      "NLP (II) Domando los Datos: Preprocesamiento y Almacenamiento en DataFrame\n",
      "\n",
      "https://josemamerto.blogspot.com/2024/05/nlp-ii-domando-los-datos.html\n",
      "----------------------\n",
      "\n",
      "NLP (I) Web Scraping\n",
      "\n",
      "https://josemamerto.blogspot.com/2024/05/nlp-i-web-scraping.html\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(requests.get('https://josemamerto.blogspot.com/').content, 'html.parser')\n",
    "\n",
    "text = soup.find('h3', class_='post-title').text\n",
    "enlace = soup.find('h3', class_='post-title').a['href']\n",
    "print(text)\n",
    "print(f'\\n{enlace}')\n",
    "print('----------------------')\n",
    "titles.append(text)\n",
    "links.append(enlace)    \n",
    "\n",
    "texts = soup.find_all('h3', class_='post-title entry-title')\n",
    "\n",
    "for i in texts:\n",
    "    print(i.text)\n",
    "    print(i.a['href'])\n",
    "    print('----------------------')\n",
    "    titles.append(i.text)\n",
    "    links.append(i.a['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limpiar titles, eliminar saltos de linea\n",
    "titles = [re.sub('\\n', '', i) for i in titles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "textos = {titulo: [] for titulo in titles}\n",
    "for titulo, link in zip(titles, links):\n",
    "    soup = BeautifulSoup(requests.get(f'{link}').content, 'html.parser')\n",
    "    articulo_div = soup.find('div', class_ = 'post-body entry-content float-container')\n",
    "    articulos = articulo_div.find_all('p')  # find all 'p' tags in the div\n",
    "    for articulo in articulos:\n",
    "        textos[titulo].append(articulo.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "for titulo in textos:\n",
    "    textos[titulo] = ' '.join(textos[titulo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Espacios Vectoriales</td>\n",
       "      <td>Explorando los Espacios Vectoriales: Fundamen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pero esto quÃ© es</td>\n",
       "      <td>AcÃ¡ en este bloggaso vamos a presentar unos d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>El pelucas</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NLP (y IV) Creando un Buscador de Pisos con Si...</td>\n",
       "      <td>En esta Ãºltima parte de nuestro anÃ¡lisis, cons...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NLP (III)  Explorando Temas Ocultos: Topic Mod...</td>\n",
       "      <td>En el anÃ¡lisis de texto, el topic modeling es ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NLP (II) Domando los Datos: Preprocesamiento y...</td>\n",
       "      <td>En el fascinante mundo del anÃ¡lisis de datos,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NLP (I) Web Scraping</td>\n",
       "      <td>Â¿Alguna vez te has preguntado cÃ³mo se recopil...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0                               Espacios Vectoriales   \n",
       "1                                   Pero esto quÃ© es   \n",
       "2                                         El pelucas   \n",
       "3  NLP (y IV) Creando un Buscador de Pisos con Si...   \n",
       "4  NLP (III)  Explorando Temas Ocultos: Topic Mod...   \n",
       "5  NLP (II) Domando los Datos: Preprocesamiento y...   \n",
       "6                               NLP (I) Web Scraping   \n",
       "\n",
       "                                                Text  \n",
       "0  Â Explorando los Espacios Vectoriales: Fundamen...  \n",
       "1  Â AcÃ¡ en este bloggaso vamos a presentar unos d...  \n",
       "2                                                     \n",
       "3  En esta Ãºltima parte de nuestro anÃ¡lisis, cons...  \n",
       "4  En el anÃ¡lisis de texto, el topic modeling es ...  \n",
       "5  Â En el fascinante mundo del anÃ¡lisis de datos,...  \n",
       "6  Â Â¿Alguna vez te has preguntado cÃ³mo se recopil...  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(list(textos.items()), columns=['Title', 'Text'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import unicodedata\n",
    "\n",
    "nlp=spacy.load('es_core_news_sm')\n",
    "\n",
    "def normalize_document(doc):\n",
    "\n",
    "    doc = unicodedata.normalize('NFKD', doc).encode('ASCII', 'ignore').decode('utf-8')\n",
    "\n",
    "    # tokenizamos el texto\n",
    "    tokens = nlp(doc)\n",
    "    # quitamos puntuaciÃ³n/espacios/stop words y cogemos el lema\n",
    "    lemmas = [t.lemma_ for t in tokens if not t.is_punct and not t.is_space and not t.is_stop]\n",
    "    doc = ' '.join(lemmas)\n",
    "    return doc\n",
    "\n",
    "def normalize_corpus(corpus):\n",
    "    \"\"\"Normaliza un corpus de documentos aplicando al funciÃ³n de normalizaciÃ³n\n",
    "    normalize_document() a cada documento de la lista pasada como argumento\"\"\"   \n",
    "    return [normalize_document(text) for text in corpus]\n",
    "\n",
    "corpus = df['Text'].tolist()\n",
    "corpus = normalize_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# usamos caracterÃ­sticas tf-idf para LSA.\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=2)\n",
    "tfidf = tfidf_vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    \"\"\"FunciÃ³n auxiliar para mostrar los tÃ©rminos mÃ¡s importantes\n",
    "    de cada topic\"\"\"\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = f\"Topic #{topic_idx}: \"\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics en modelo LSA:\n",
      "Topic #0: dato piso paso web scraping\n",
      "Topic #1: word texto paso embeddings analisis\n",
      "Topic #2: vector propiedad aplicaciÃ³n conjunto word\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation\n",
    "\n",
    "# Ajustamos el modelo LSA\n",
    "lsa = TruncatedSVD(n_components=3).fit(tfidf)\n",
    "\n",
    "print(\"\\nTopics en modelo LSA:\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "print_top_words(lsa, tfidf_feature_names, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>Topic 0</th>\n",
       "      <th>Topic 1</th>\n",
       "      <th>Topic 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Espacios Vectoriales</td>\n",
       "      <td>Explorando los Espacios Vectoriales: Fundamen...</td>\n",
       "      <td>0.3631</td>\n",
       "      <td>-0.5178</td>\n",
       "      <td>0.6996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pero esto quÃ© es</td>\n",
       "      <td>AcÃ¡ en este bloggaso vamos a presentar unos d...</td>\n",
       "      <td>0.6585</td>\n",
       "      <td>-0.4431</td>\n",
       "      <td>-0.1216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>El pelucas</td>\n",
       "      <td></td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NLP (y IV) Creando un Buscador de Pisos con Si...</td>\n",
       "      <td>En esta Ãºltima parte de nuestro anÃ¡lisis, cons...</td>\n",
       "      <td>0.6331</td>\n",
       "      <td>0.0632</td>\n",
       "      <td>0.0645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NLP (III)  Explorando Temas Ocultos: Topic Mod...</td>\n",
       "      <td>En el anÃ¡lisis de texto, el topic modeling es ...</td>\n",
       "      <td>0.5704</td>\n",
       "      <td>0.6455</td>\n",
       "      <td>0.3228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NLP (II) Domando los Datos: Preprocesamiento y...</td>\n",
       "      <td>En el fascinante mundo del anÃ¡lisis de datos,...</td>\n",
       "      <td>0.7538</td>\n",
       "      <td>0.2713</td>\n",
       "      <td>-0.1401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NLP (I) Web Scraping</td>\n",
       "      <td>Â¿Alguna vez te has preguntado cÃ³mo se recopil...</td>\n",
       "      <td>0.5397</td>\n",
       "      <td>-0.2464</td>\n",
       "      <td>-0.5435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0                               Espacios Vectoriales   \n",
       "1                                   Pero esto quÃ© es   \n",
       "2                                         El pelucas   \n",
       "3  NLP (y IV) Creando un Buscador de Pisos con Si...   \n",
       "4  NLP (III)  Explorando Temas Ocultos: Topic Mod...   \n",
       "5  NLP (II) Domando los Datos: Preprocesamiento y...   \n",
       "6                               NLP (I) Web Scraping   \n",
       "\n",
       "                                                Text  Topic 0  Topic 1  \\\n",
       "0  Â Explorando los Espacios Vectoriales: Fundamen...   0.3631  -0.5178   \n",
       "1  Â AcÃ¡ en este bloggaso vamos a presentar unos d...   0.6585  -0.4431   \n",
       "2                                                      0.0000   0.0000   \n",
       "3  En esta Ãºltima parte de nuestro anÃ¡lisis, cons...   0.6331   0.0632   \n",
       "4  En el anÃ¡lisis de texto, el topic modeling es ...   0.5704   0.6455   \n",
       "5  Â En el fascinante mundo del anÃ¡lisis de datos,...   0.7538   0.2713   \n",
       "6  Â Â¿Alguna vez te has preguntado cÃ³mo se recopil...   0.5397  -0.2464   \n",
       "\n",
       "   Topic 2  \n",
       "0   0.6996  \n",
       "1  -0.1216  \n",
       "2   0.0000  \n",
       "3   0.0645  \n",
       "4   0.3228  \n",
       "5  -0.1401  \n",
       "6  -0.5435  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assuming lsa and tfidf are defined and have the correct dimensions\n",
    "topic_values = np.round(lsa.transform(tfidf), 4)\n",
    "\n",
    "# convert the array to a DataFrame\n",
    "topic_df = pd.DataFrame(topic_values, columns=['Topic 0', 'Topic 1', 'Topic 2'])\n",
    "\n",
    "# concatenate the DataFrames\n",
    "df = pd.concat([df, topic_df], axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://josemamerto.blogspot.com/2024/05/espacios-vectoriales.html\n",
      "alex es mi lider\n",
      "chÃ© me cague a trompadas de risa con el content, publicÃ¡ mÃ¡s xfiiiiiiiii\n",
      "----------------------\n",
      "https://josemamerto.blogspot.com/2024/05/espacios-vectoriales.html\n",
      "Elseroculto\n",
      "ç©ºé—´å‘é‡æ˜¯çº¿æ€§ä»£æ•°ç ”ç©¶çš„åŸºçŸ³ï¼Œè¿™ä¸€å­¦ç§‘åœ¨ç‰©ç†å­¦ã€å·¥ç¨‹å­¦ã€è®¡ç®—æœºç§‘å­¦å’Œç»æµå­¦ç­‰é¢†åŸŸæœ‰å¹¿æ³›çš„åº”ç”¨ã€‚åœ¨è¿™ç¯‡åšå®¢ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨ç©ºé—´å‘é‡çš„ä¸–ç•Œï¼Œæ¢ç´¢å…¶åŸºç¡€ã€ç‰¹æ€§å’Œåº”ç”¨ã€‚ä»€ä¹ˆæ˜¯ç©ºé—´å‘é‡ï¼Ÿç©ºé—´å‘é‡æ˜¯ä¸€ç§æ•°å­¦ç»“æ„ï¼Œç”±ä¸€ç»„ç§°ä¸ºå‘é‡çš„å…ƒç´ ä»¥åŠä¸¤ç§æ“ä½œç»„æˆï¼Œå³å‘é‡åŠ æ³•å’Œæ ‡é‡ä¹˜æ³•ï¼Œæ»¡è¶³æŸäº›åŸºæœ¬æ€§è´¨ã€‚æ­£å¼åœ°ï¼Œç©ºé—´å‘é‡ğ‘‰V åœ¨åŸŸğ¹F ä¸Šæ˜¯ä¸€ä¸ªéç©ºå…ƒç´ é›†åˆï¼Œç§°ä¸ºå‘é‡ï¼Œä»¥åŠä¸¤ç§åœ¨ğ‘‰V ä¸Šå®šä¹‰çš„æ“ä½œï¼šå‘é‡åŠ æ³•å’Œæ ‡é‡ä¹˜æ³•ï¼Œè¿™äº›æ“ä½œæ»¡è¶³å…«æ¡å…¬ç†ï¼Œè§„å®šäº†å®ƒä»¬å¿…é¡»æ»¡è¶³çš„åŸºæœ¬æ€§è´¨ã€‚ç‰¹æ€§å’Œç¤ºä¾‹ç©ºé—´å‘é‡å¯ä»¥å…·æœ‰å¤šç§ç‰¹æ€§ï¼Œå…¶ä¸­ä¸€äº›åŒ…æ‹¬å¯¹åŠ æ³•å’Œæ ‡é‡ä¹˜æ³•å°é—­ã€å­˜åœ¨åŠ æ³•å•ä½å…ƒç´ ã€å­˜åœ¨åŠ æ³•é€†å…ƒç´ å’Œæ“ä½œçš„ç»“åˆå¾‹ã€‚å¸¸è§çš„ç©ºé—´å‘é‡ç¤ºä¾‹åŒ…æ‹¬æ¬§å‡ é‡Œå¾—ç©ºé—´ğ‘…ğ‘›ï¼Œå…¶ä¸­åŒ…å«æ‰€æœ‰n-ç»´å‘é‡ï¼Œå…¶åæ ‡ä¸ºå®æ•°ï¼Œä»¥åŠğ‘ƒğ‘›å¤šé¡¹å¼ç©ºé—´ï¼Œå…¶å¤šé¡¹å¼çš„æ¬¡æ•°ä¸è¶…è¿‡nã€‚åœ¨ç°å®ç”Ÿæ´»ä¸­çš„åº”ç”¨ç©ºé—´å‘é‡åœ¨ç°å®ä¸–ç•Œä¸­æœ‰å¹¿æ³›çš„åº”ç”¨ã€‚åœ¨ç‰©ç†å­¦ä¸­ï¼Œå®ƒä»¬ç”¨äºå»ºæ¨¡åŠ›ã€åŠ›çŸ©å’Œç”µç£åœºã€‚åœ¨å·¥ç¨‹å­¦ä¸­ï¼Œå®ƒä»¬åº”ç”¨äºç»“æ„åˆ†æã€ç”µè·¯å’Œæ§åˆ¶ç³»ç»Ÿã€‚åœ¨è®¡ç®—æœºç§‘å­¦ä¸­ï¼Œå®ƒä»¬ç”¨äºå›¾åƒå¤„ç†ã€è®¡ç®—æœºå›¾å½¢å­¦å’Œæœºå™¨å­¦ä¹ ã€‚ç”šè‡³åœ¨ç»æµå­¦ä¸­ï¼Œå®ƒä»¬ç”¨äºå»ºæ¨¡ç»æµå˜é‡ä¹‹é—´çš„å…³ç³»å’Œä¼˜åŒ–è´¢åŠ¡å†³ç­–ã€‚é«˜çº§ç‰¹æ€§å’Œå®šç†é™¤äº†åŸºæœ¬æ€§è´¨å¤–ï¼Œç©ºé—´å‘é‡è¿˜è¡¨ç°å‡ºä¸€ç³»åˆ—æ›´é«˜çº§çš„ç‰¹æ€§å’Œé‡è¦å®šç†ã€‚ä¾‹å¦‚ï¼Œå…±åŒåŸºç¡€å®šç†è§„å®šæ‰€æœ‰ç©ºé—´å‘é‡éƒ½æœ‰ä¸€ä¸ªåŸºç¡€ï¼Œå³ä¸€ç»„çº¿æ€§ç‹¬ç«‹çš„å‘é‡ï¼Œå®ƒä»¬ç”Ÿæˆæ•´ä¸ªç©ºé—´ã€‚ç§©å®šç†å‘Šè¯‰æˆ‘ä»¬çŸ©é˜µçš„ç§©ç­‰äºå¯ä»¥ä»å…¶åˆ—ä¸­è·å¾—çš„çº¿æ€§ç‹¬ç«‹å‘é‡çš„æœ€å¤§æ•°é‡ã€‚ç»“è®ºç©ºé—´å‘é‡æ˜¯æ•°å­¦å’Œåº”ç”¨ç§‘å­¦ä¸­å¼ºå¤§è€Œå¤šæ‰å¤šè‰ºçš„å·¥å…·ã€‚å®ƒä»¬çš„ç ”ç©¶ä¸ä»…æä¾›äº†å¯¹ç©ºé—´ç»“æ„å’Œçº¿æ€§å˜æ¢çš„æ·±å…¥ç†è§£ï¼Œè€Œä¸”åœ¨å„ç§é¢†åŸŸéƒ½æœ‰å®é™…åº”ç”¨ã€‚æ— è®ºæ˜¯å¯¹äºšåŸå­ç²’å­è¿åŠ¨å»ºæ¨¡è¿˜æ˜¯è®¾è®¡äººå·¥æ™ºèƒ½ç®—æ³•ï¼Œç©ºé—´å‘é‡ä»ç„¶æ˜¯æˆ‘ä»¬å¯¹å‘¨å›´ä¸–ç•Œç†è§£çš„ä¸å¯æˆ–ç¼ºçš„ä¸€éƒ¨åˆ†ã€‚å¸Œæœ›è¿™æ¬¡æ¢ç´¢ç»™æ‚¨å¸¦æ¥äº†å¯¹ç©ºé—´å‘é‡é‡è¦æ€§å’Œç¾å¦™ä¹‹å¤„çš„æ¸…æ™°è®¤è¯†ï¼å¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜æˆ–è¯„è®ºï¼Œè¯·éšæ—¶åˆ†äº«ï¼\n",
      "----------------------\n",
      "https://josemamerto.blogspot.com/2024/05/pero-esto-que-es.html\n",
      "Elseroculto\n",
      "Si quieren contratar los servicios de este sicario llamen al nÃºmero que aparece a continuaciÃ³n 922 999 999 999 999\n",
      "----------------------\n",
      "https://josemamerto.blogspot.com/2024/03/blog-post.html\n",
      "Elseroculto\n",
      "Cara de asesino en serie pero recomendable muy buenos servicios.\n",
      "----------------------\n",
      "https://josemamerto.blogspot.com/2024/03/blog-post.html\n",
      "elkebabesgrande\n",
      "Quien pudiera\n",
      "----------------------\n",
      "https://josemamerto.blogspot.com/2024/03/blog-post.html\n",
      "Carlos Peris\n",
      "Que miedo!\n",
      "----------------------\n",
      "https://josemamerto.blogspot.com/2024/03/blog-post.html\n",
      "Slanc\n",
      "Â¿Estas siendo internacionalmente buscado por la INTERPOL?\n",
      "----------------------\n",
      "https://josemamerto.blogspot.com/2024/03/blog-post.html\n",
      "Mr_pollon2000\n",
      "Buen articulo sobre el web scraping, un saludo de tu amigo Scalving\n",
      "----------------------\n",
      "https://josemamerto.blogspot.com/2024/05/nlp-y-iv-creando-un-buscador-de-pisos.html\n",
      "Elseroculto\n",
      "Â¡Este proyecto suena emocionante! Crear un buscador de pisos que utilice la similitud de coseno para comparar la entrada del usuario con las descripciones disponibles es una excelente manera de ofrecer resultados relevantes y personalizados. AdemÃ¡s, la integraciÃ³n con la API para automatizar el proceso es un gran paso hacia la eficiencia.El uso de la similitud de coseno es inteligente, ya que permite una comparaciÃ³n precisa entre la consulta del usuario y las descripciones de los pisos. Y la integraciÃ³n del modelo LLaMA para el procesamiento de datos agrega una capa adicional de sofisticaciÃ³n al filtrar la informaciÃ³n y crear word embeddings para evaluar la similitud.En resumen, has creado una herramienta poderosa que no solo facilitarÃ¡ la bÃºsqueda de viviendas para los usuarios, sino que tambiÃ©n les proporcionarÃ¡ resultados relevantes y precisos. Â¡Excelente trabajo! ğŸ˜ŠğŸ \n",
      "----------------------\n",
      "https://josemamerto.blogspot.com/2024/05/nlp-iii-explorando-temas-ocultos-topic.html\n",
      "Elseroculto\n",
      "Â¡Este artÃ­culo sobre NLP es muy informativo y ofrece una visiÃ³n fascinante sobre el uso del topic modeling y los word embeddings para analizar datos de texto!El topic modeling es una tÃ©cnica increÃ­blemente Ãºtil para descubrir temas subyacentes en conjuntos de documentos, lo que puede proporcionar insights valiosos sobre los datos. La forma en que el artÃ­culo lo presenta y explica cÃ³mo funciona es muy clara y comprensible.AdemÃ¡s, la incorporaciÃ³n de los word embeddings mediante Word2Vec es una adiciÃ³n excelente. Estos embeddings permiten representar palabras de manera semÃ¡ntica, lo que enriquece aÃºn mÃ¡s el anÃ¡lisis de texto al capturar las relaciones semÃ¡nticas entre las palabras.El hecho de que el artÃ­culo incluya ejemplos de cÃ³digo para implementar estas tÃ©cnicas en Python usando bibliotecas como Gensim es muy Ãºtil para aquellos que deseen aplicar estos mÃ©todos en sus propios proyectos de NLP.En resumen, este artÃ­culo proporciona una introducciÃ³n sÃ³lida al topic modeling y los word embeddings, y muestra cÃ³mo estas tÃ©cnicas pueden ser aplicadas de manera efectiva para comprender y analizar datos de texto de manera mÃ¡s profunda. Â¡Excelente trabajo! ğŸŒŸ\n",
      "----------------------\n",
      "https://josemamerto.blogspot.com/2024/05/nlp-ii-domando-los-datos.html\n",
      "Samuel\n",
      "Â¡Hola! Me encantÃ³ tu artÃ­culo sobre el preprocesamiento y almacenamiento de datos en un DataFrame utilizando Python y pandas. El proceso paso a paso que proporcionaste es muy claro y Ãºtil para cualquiera que estÃ© comenzando en el mundo del anÃ¡lisis de datos.Me gustÃ³ especialmente cÃ³mo abordaste la limpieza de datos y el preprocesamiento de texto, mostrando cÃ³mo eliminar valores nulos, duplicados y caracteres especiales, asÃ­ como convertir el texto a minÃºsculas y eliminar palabras vacÃ­as. Estos son pasos esenciales para garantizar la calidad de los datos antes de realizar cualquier anÃ¡lisis.AdemÃ¡s, la secciÃ³n sobre la importaciÃ³n de un archivo pickle y su transformaciÃ³n en un DataFrame de pandas fue muy informativa. La forma en que explicaste cada paso y proporcionaste ejemplos de cÃ³digo fue muy Ãºtil para comprender el proceso.En general, creo que tu artÃ­culo proporciona una excelente introducciÃ³n al preprocesamiento de datos y el manejo de DataFrames en Python, y estoy seguro de que serÃ¡ de gran ayuda para muchos lectores interesados en aprender anÃ¡lisis de datos. Â¡Espero con ansias tu prÃ³ximo artÃ­culo!\n",
      "----------------------\n",
      "https://josemamerto.blogspot.com/2024/05/nlp-ii-domando-los-datos.html\n",
      "lentejas\n",
      "como dijo Mariano Rajoy: it's very difficult todo esto\n",
      "----------------------\n",
      "https://josemamerto.blogspot.com/2024/05/nlp-ii-domando-los-datos.html\n",
      "Chegio\n",
      "Segun la teorÃ­a del Big Crunch, con el tiempo, tras el Big Bang, todo el universo volverÃ¡ a unirse en solo punto como estaba antes del Big Bang. Esto podrÃ­a abogar a un universo \"pulsante\" donde surgiesen diversos Big Bangs continuamente tras los Big Crunchs. Si ademÃ¡s sugerimos un universo determinista, esto implicarÃ­a que podriamos determinar cualquier suceso del universo sabiendo lo que ya ha pasado. Por lo que nuestras acciones serÃ­an fijas y podrian predecirse. La implicaciÃ³n de este detreminismo harÃ­a en todos y cada uno de los universos tomaramos las mismas decisiones y repitiesemos la misma vida una y otra vez. Y en todos y cada uno de estos infinitos universos se toma la decisiÃ³n de crear este magnifico Blog, muchas gracias.\n",
      "----------------------\n",
      "https://josemamerto.blogspot.com/2024/05/nlp-i-web-scraping.html\n",
      "Samuel\n",
      "Â¡QuÃ© artÃ­culo tan fascinante sobre el web scraping! Me encantÃ³ cÃ³mo proporcionaste una visiÃ³n completa de esta tÃ©cnica increÃ­ble que nos permite extraer datos de la web de manera automatizada. El mundo del web scraping es realmente emocionante y lleno de posibilidades para explorar y aprovechar la vasta cantidad de informaciÃ³n disponible en Internet.Tu explicaciÃ³n sobre quÃ© es el web scraping y cÃ³mo funciona fue muy clara y comprensible. Me gustÃ³ especialmente cÃ³mo diste ejemplos prÃ¡cticos, como buscar informaciÃ³n sobre el clima en diferentes ciudades, para ilustrar el proceso. TambiÃ©n aprecio que mencionaras las herramientas y tecnologÃ­as mÃ¡s populares utilizadas en el web scraping, como BeautifulSoup y Scrapy, lo que hace que sea mÃ¡s accesible para aquellos que estÃ©n interesados en probarlo.AdemÃ¡s, tu artÃ­culo abordÃ³ de manera adecuada las consideraciones Ã©ticas y legales asociadas con el web scraping. Es crucial respetar los tÃ©rminos de servicio de los sitios web y obtener permiso antes de extraer datos, y aprecio que hayas destacado este punto.Por Ãºltimo, tu explicaciÃ³n sobre cÃ³mo implementaste el web scraping utilizando las bibliotecas Selenium y BeautifulSoup para acceder a las pÃ¡ginas web y extraer informaciÃ³n fue muy informativa. Proporcionaste una visiÃ³n interna de cÃ³mo se realiza el proceso en la prÃ¡ctica, lo que ayuda a comprender mejor el alcance y las posibilidades del web scraping.En resumen, tu artÃ­culo proporciona una excelente introducciÃ³n al mundo del web scraping y ofrece una visiÃ³n completa de su funcionamiento, aplicaciones y consideraciones Ã©ticas. Â¡Espero ver mÃ¡s contenido como este en el futuro!\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "keys = links\n",
    "comentarios = {key: {} for key in keys}\n",
    "\n",
    "for i in keys:\n",
    "    soup = BeautifulSoup(requests.get(f'{i}').content, 'html.parser')\n",
    "    comentario = soup.find_all('li', class_ = 'comment')\n",
    "    for j in comentario:\n",
    "        try :\n",
    "            comments = j.find('p', class_ = 'comment-content')\n",
    "            user = j.find('cite', class_ = 'user')\n",
    "            print(i)\n",
    "            print(user.text)\n",
    "            print(comments.text)\n",
    "            print('----------------------')\n",
    "            comentarios[i][user.text] = comments.text\n",
    "        except AttributeError:\n",
    "            comments = 'No hay comentarios'\n",
    "            print(i)\n",
    "            print(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'https://josemamerto.blogspot.com/2024/05/espacios-vectoriales.html': {'alex es mi lider': 'chÃ© me cague a trompadas de risa con el content, publicÃ¡ mÃ¡s xfiiiiiiiii',\n",
       "  'Elseroculto': 'ç©ºé—´å‘é‡æ˜¯çº¿æ€§ä»£æ•°ç ”ç©¶çš„åŸºçŸ³ï¼Œè¿™ä¸€å­¦ç§‘åœ¨ç‰©ç†å­¦ã€å·¥ç¨‹å­¦ã€è®¡ç®—æœºç§‘å­¦å’Œç»æµå­¦ç­‰é¢†åŸŸæœ‰å¹¿æ³›çš„åº”ç”¨ã€‚åœ¨è¿™ç¯‡åšå®¢ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨ç©ºé—´å‘é‡çš„ä¸–ç•Œï¼Œæ¢ç´¢å…¶åŸºç¡€ã€ç‰¹æ€§å’Œåº”ç”¨ã€‚ä»€ä¹ˆæ˜¯ç©ºé—´å‘é‡ï¼Ÿç©ºé—´å‘é‡æ˜¯ä¸€ç§æ•°å­¦ç»“æ„ï¼Œç”±ä¸€ç»„ç§°ä¸ºå‘é‡çš„å…ƒç´ ä»¥åŠä¸¤ç§æ“ä½œç»„æˆï¼Œå³å‘é‡åŠ æ³•å’Œæ ‡é‡ä¹˜æ³•ï¼Œæ»¡è¶³æŸäº›åŸºæœ¬æ€§è´¨ã€‚æ­£å¼åœ°ï¼Œç©ºé—´å‘é‡ğ‘‰V åœ¨åŸŸğ¹F ä¸Šæ˜¯ä¸€ä¸ªéç©ºå…ƒç´ é›†åˆï¼Œç§°ä¸ºå‘é‡ï¼Œä»¥åŠä¸¤ç§åœ¨ğ‘‰V ä¸Šå®šä¹‰çš„æ“ä½œï¼šå‘é‡åŠ æ³•å’Œæ ‡é‡ä¹˜æ³•ï¼Œè¿™äº›æ“ä½œæ»¡è¶³å…«æ¡å…¬ç†ï¼Œè§„å®šäº†å®ƒä»¬å¿…é¡»æ»¡è¶³çš„åŸºæœ¬æ€§è´¨ã€‚ç‰¹æ€§å’Œç¤ºä¾‹ç©ºé—´å‘é‡å¯ä»¥å…·æœ‰å¤šç§ç‰¹æ€§ï¼Œå…¶ä¸­ä¸€äº›åŒ…æ‹¬å¯¹åŠ æ³•å’Œæ ‡é‡ä¹˜æ³•å°é—­ã€å­˜åœ¨åŠ æ³•å•ä½å…ƒç´ ã€å­˜åœ¨åŠ æ³•é€†å…ƒç´ å’Œæ“ä½œçš„ç»“åˆå¾‹ã€‚å¸¸è§çš„ç©ºé—´å‘é‡ç¤ºä¾‹åŒ…æ‹¬æ¬§å‡ é‡Œå¾—ç©ºé—´ğ‘…ğ‘›ï¼Œå…¶ä¸­åŒ…å«æ‰€æœ‰n-ç»´å‘é‡ï¼Œå…¶åæ ‡ä¸ºå®æ•°ï¼Œä»¥åŠğ‘ƒğ‘›å¤šé¡¹å¼ç©ºé—´ï¼Œå…¶å¤šé¡¹å¼çš„æ¬¡æ•°ä¸è¶…è¿‡nã€‚åœ¨ç°å®ç”Ÿæ´»ä¸­çš„åº”ç”¨ç©ºé—´å‘é‡åœ¨ç°å®ä¸–ç•Œä¸­æœ‰å¹¿æ³›çš„åº”ç”¨ã€‚åœ¨ç‰©ç†å­¦ä¸­ï¼Œå®ƒä»¬ç”¨äºå»ºæ¨¡åŠ›ã€åŠ›çŸ©å’Œç”µç£åœºã€‚åœ¨å·¥ç¨‹å­¦ä¸­ï¼Œå®ƒä»¬åº”ç”¨äºç»“æ„åˆ†æã€ç”µè·¯å’Œæ§åˆ¶ç³»ç»Ÿã€‚åœ¨è®¡ç®—æœºç§‘å­¦ä¸­ï¼Œå®ƒä»¬ç”¨äºå›¾åƒå¤„ç†ã€è®¡ç®—æœºå›¾å½¢å­¦å’Œæœºå™¨å­¦ä¹ ã€‚ç”šè‡³åœ¨ç»æµå­¦ä¸­ï¼Œå®ƒä»¬ç”¨äºå»ºæ¨¡ç»æµå˜é‡ä¹‹é—´çš„å…³ç³»å’Œä¼˜åŒ–è´¢åŠ¡å†³ç­–ã€‚é«˜çº§ç‰¹æ€§å’Œå®šç†é™¤äº†åŸºæœ¬æ€§è´¨å¤–ï¼Œç©ºé—´å‘é‡è¿˜è¡¨ç°å‡ºä¸€ç³»åˆ—æ›´é«˜çº§çš„ç‰¹æ€§å’Œé‡è¦å®šç†ã€‚ä¾‹å¦‚ï¼Œå…±åŒåŸºç¡€å®šç†è§„å®šæ‰€æœ‰ç©ºé—´å‘é‡éƒ½æœ‰ä¸€ä¸ªåŸºç¡€ï¼Œå³ä¸€ç»„çº¿æ€§ç‹¬ç«‹çš„å‘é‡ï¼Œå®ƒä»¬ç”Ÿæˆæ•´ä¸ªç©ºé—´ã€‚ç§©å®šç†å‘Šè¯‰æˆ‘ä»¬çŸ©é˜µçš„ç§©ç­‰äºå¯ä»¥ä»å…¶åˆ—ä¸­è·å¾—çš„çº¿æ€§ç‹¬ç«‹å‘é‡çš„æœ€å¤§æ•°é‡ã€‚ç»“è®ºç©ºé—´å‘é‡æ˜¯æ•°å­¦å’Œåº”ç”¨ç§‘å­¦ä¸­å¼ºå¤§è€Œå¤šæ‰å¤šè‰ºçš„å·¥å…·ã€‚å®ƒä»¬çš„ç ”ç©¶ä¸ä»…æä¾›äº†å¯¹ç©ºé—´ç»“æ„å’Œçº¿æ€§å˜æ¢çš„æ·±å…¥ç†è§£ï¼Œè€Œä¸”åœ¨å„ç§é¢†åŸŸéƒ½æœ‰å®é™…åº”ç”¨ã€‚æ— è®ºæ˜¯å¯¹äºšåŸå­ç²’å­è¿åŠ¨å»ºæ¨¡è¿˜æ˜¯è®¾è®¡äººå·¥æ™ºèƒ½ç®—æ³•ï¼Œç©ºé—´å‘é‡ä»ç„¶æ˜¯æˆ‘ä»¬å¯¹å‘¨å›´ä¸–ç•Œç†è§£çš„ä¸å¯æˆ–ç¼ºçš„ä¸€éƒ¨åˆ†ã€‚å¸Œæœ›è¿™æ¬¡æ¢ç´¢ç»™æ‚¨å¸¦æ¥äº†å¯¹ç©ºé—´å‘é‡é‡è¦æ€§å’Œç¾å¦™ä¹‹å¤„çš„æ¸…æ™°è®¤è¯†ï¼å¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜æˆ–è¯„è®ºï¼Œè¯·éšæ—¶åˆ†äº«ï¼'},\n",
       " 'https://josemamerto.blogspot.com/2024/05/pero-esto-que-es.html': {'Elseroculto': 'Si quieren contratar los servicios de este sicario llamen al nÃºmero que aparece a continuaciÃ³n 922 999 999 999 999'},\n",
       " 'https://josemamerto.blogspot.com/2024/03/blog-post.html': {'Elseroculto': 'Cara de asesino en serie pero recomendable muy buenos servicios.',\n",
       "  'elkebabesgrande': 'Quien pudiera',\n",
       "  'Carlos Peris': 'Que miedo!',\n",
       "  'Slanc': 'Â¿Estas siendo internacionalmente buscado por la INTERPOL?',\n",
       "  'Mr_pollon2000': 'Buen articulo sobre el web scraping, un saludo de tu amigo Scalving'},\n",
       " 'https://josemamerto.blogspot.com/2024/05/nlp-y-iv-creando-un-buscador-de-pisos.html': {'Elseroculto': 'Â¡Este proyecto suena emocionante! Crear un buscador de pisos que utilice la similitud de coseno para comparar la entrada del usuario con las descripciones disponibles es una excelente manera de ofrecer resultados relevantes y personalizados. AdemÃ¡s, la integraciÃ³n con la API para automatizar el proceso es un gran paso hacia la eficiencia.El uso de la similitud de coseno es inteligente, ya que permite una comparaciÃ³n precisa entre la consulta del usuario y las descripciones de los pisos. Y la integraciÃ³n del modelo LLaMA para el procesamiento de datos agrega una capa adicional de sofisticaciÃ³n al filtrar la informaciÃ³n y crear word embeddings para evaluar la similitud.En resumen, has creado una herramienta poderosa que no solo facilitarÃ¡ la bÃºsqueda de viviendas para los usuarios, sino que tambiÃ©n les proporcionarÃ¡ resultados relevantes y precisos. Â¡Excelente trabajo! ğŸ˜ŠğŸ '},\n",
       " 'https://josemamerto.blogspot.com/2024/05/nlp-iii-explorando-temas-ocultos-topic.html': {'Elseroculto': 'Â¡Este artÃ­culo sobre NLP es muy informativo y ofrece una visiÃ³n fascinante sobre el uso del topic modeling y los word embeddings para analizar datos de texto!El topic modeling es una tÃ©cnica increÃ­blemente Ãºtil para descubrir temas subyacentes en conjuntos de documentos, lo que puede proporcionar insights valiosos sobre los datos. La forma en que el artÃ­culo lo presenta y explica cÃ³mo funciona es muy clara y comprensible.AdemÃ¡s, la incorporaciÃ³n de los word embeddings mediante Word2Vec es una adiciÃ³n excelente. Estos embeddings permiten representar palabras de manera semÃ¡ntica, lo que enriquece aÃºn mÃ¡s el anÃ¡lisis de texto al capturar las relaciones semÃ¡nticas entre las palabras.El hecho de que el artÃ­culo incluya ejemplos de cÃ³digo para implementar estas tÃ©cnicas en Python usando bibliotecas como Gensim es muy Ãºtil para aquellos que deseen aplicar estos mÃ©todos en sus propios proyectos de NLP.En resumen, este artÃ­culo proporciona una introducciÃ³n sÃ³lida al topic modeling y los word embeddings, y muestra cÃ³mo estas tÃ©cnicas pueden ser aplicadas de manera efectiva para comprender y analizar datos de texto de manera mÃ¡s profunda. Â¡Excelente trabajo! ğŸŒŸ'},\n",
       " 'https://josemamerto.blogspot.com/2024/05/nlp-ii-domando-los-datos.html': {'Samuel': 'Â¡Hola! Me encantÃ³ tu artÃ­culo sobre el preprocesamiento y almacenamiento de datos en un DataFrame utilizando Python y pandas. El proceso paso a paso que proporcionaste es muy claro y Ãºtil para cualquiera que estÃ© comenzando en el mundo del anÃ¡lisis de datos.Me gustÃ³ especialmente cÃ³mo abordaste la limpieza de datos y el preprocesamiento de texto, mostrando cÃ³mo eliminar valores nulos, duplicados y caracteres especiales, asÃ­ como convertir el texto a minÃºsculas y eliminar palabras vacÃ­as. Estos son pasos esenciales para garantizar la calidad de los datos antes de realizar cualquier anÃ¡lisis.AdemÃ¡s, la secciÃ³n sobre la importaciÃ³n de un archivo pickle y su transformaciÃ³n en un DataFrame de pandas fue muy informativa. La forma en que explicaste cada paso y proporcionaste ejemplos de cÃ³digo fue muy Ãºtil para comprender el proceso.En general, creo que tu artÃ­culo proporciona una excelente introducciÃ³n al preprocesamiento de datos y el manejo de DataFrames en Python, y estoy seguro de que serÃ¡ de gran ayuda para muchos lectores interesados en aprender anÃ¡lisis de datos. Â¡Espero con ansias tu prÃ³ximo artÃ­culo!',\n",
       "  'lentejas': \"como dijo Mariano Rajoy: it's very difficult todo esto\",\n",
       "  'Chegio': 'Segun la teorÃ­a del Big Crunch, con el tiempo, tras el Big Bang, todo el universo volverÃ¡ a unirse en solo punto como estaba antes del Big Bang. Esto podrÃ­a abogar a un universo \"pulsante\" donde surgiesen diversos Big Bangs continuamente tras los Big Crunchs. Si ademÃ¡s sugerimos un universo determinista, esto implicarÃ­a que podriamos determinar cualquier suceso del universo sabiendo lo que ya ha pasado. Por lo que nuestras acciones serÃ­an fijas y podrian predecirse. La implicaciÃ³n de este detreminismo harÃ­a en todos y cada uno de los universos tomaramos las mismas decisiones y repitiesemos la misma vida una y otra vez. Y en todos y cada uno de estos infinitos universos se toma la decisiÃ³n de crear este magnifico Blog, muchas gracias.'},\n",
       " 'https://josemamerto.blogspot.com/2024/05/nlp-i-web-scraping.html': {'Samuel': 'Â¡QuÃ© artÃ­culo tan fascinante sobre el web scraping! Me encantÃ³ cÃ³mo proporcionaste una visiÃ³n completa de esta tÃ©cnica increÃ­ble que nos permite extraer datos de la web de manera automatizada. El mundo del web scraping es realmente emocionante y lleno de posibilidades para explorar y aprovechar la vasta cantidad de informaciÃ³n disponible en Internet.Tu explicaciÃ³n sobre quÃ© es el web scraping y cÃ³mo funciona fue muy clara y comprensible. Me gustÃ³ especialmente cÃ³mo diste ejemplos prÃ¡cticos, como buscar informaciÃ³n sobre el clima en diferentes ciudades, para ilustrar el proceso. TambiÃ©n aprecio que mencionaras las herramientas y tecnologÃ­as mÃ¡s populares utilizadas en el web scraping, como BeautifulSoup y Scrapy, lo que hace que sea mÃ¡s accesible para aquellos que estÃ©n interesados en probarlo.AdemÃ¡s, tu artÃ­culo abordÃ³ de manera adecuada las consideraciones Ã©ticas y legales asociadas con el web scraping. Es crucial respetar los tÃ©rminos de servicio de los sitios web y obtener permiso antes de extraer datos, y aprecio que hayas destacado este punto.Por Ãºltimo, tu explicaciÃ³n sobre cÃ³mo implementaste el web scraping utilizando las bibliotecas Selenium y BeautifulSoup para acceder a las pÃ¡ginas web y extraer informaciÃ³n fue muy informativa. Proporcionaste una visiÃ³n interna de cÃ³mo se realiza el proceso en la prÃ¡ctica, lo que ayuda a comprender mejor el alcance y las posibilidades del web scraping.En resumen, tu artÃ­culo proporciona una excelente introducciÃ³n al mundo del web scraping y ofrece una visiÃ³n completa de su funcionamiento, aplicaciones y consideraciones Ã©ticas. Â¡Espero ver mÃ¡s contenido como este en el futuro!'}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comentarios"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
